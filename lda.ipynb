{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from  sklearn.datasets import load_svmlight_file as a\n",
    "from scipy.sparse import coo_matrix, vstack\n",
    "from sklearn import svm \n",
    "from sklearn import cross_validation \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn import preprocessing\n",
    "from matplotlib.mlab import PCA as mlabPCA\n",
    "from pylab import show,title,plot\n",
    "from sklearn.lda import LDA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load all Datasets\n",
    "Data1=a(\"C:\\Users\\Sanju\\Desktop\\datasets\\TV_News_Channel_Commercial_Detection_Dataset\\BBC.txt\",n_features=4125,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False)\n",
    "Data2=a(\"C:\\Users\\Sanju\\Desktop\\datasets\\TV_News_Channel_Commercial_Detection_Dataset\\CNN.txt\",n_features=4125,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False)\n",
    "Data3=a(\"C:\\Users\\Sanju\\Desktop\\datasets\\TV_News_Channel_Commercial_Detection_Dataset\\CNNIBN.txt\",n_features=4125,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False)\n",
    "Data4=a(\"C:\\Users\\Sanju\\Desktop\\datasets\\TV_News_Channel_Commercial_Detection_Dataset\\NDTV.txt\",n_features=4125,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False)\n",
    "Data5=a(\"C:\\Users\\Sanju\\Desktop\\datasets\\TV_News_Channel_Commercial_Detection_Dataset\\TIMESNOW.txt\",n_features=4125,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129685L, 4125L)\n",
      "(129685L,)\n"
     ]
    }
   ],
   "source": [
    "#x is an sparse matrix(n_samples.n_features), (y is a list of n_samples)\n",
    "data1x,data1y=Data1\n",
    "data2x,data2y=Data2\n",
    "data3x,data3y=Data3\n",
    "data4x,data4y=Data4\n",
    "data5x,data5y=Data5\n",
    "\n",
    "x1=coo_matrix(data1x)\n",
    "x2=coo_matrix(data2x)\n",
    "x3=coo_matrix(data3x)\n",
    "x4=coo_matrix(data4x)\n",
    "x5=coo_matrix(data5x)\n",
    "\n",
    "data_X=vstack([x1,x2,x3,x4,x5]).toarray()\n",
    "\n",
    "print data_X.shape\n",
    "\n",
    "Data_Y=np.concatenate((data1y,data2y,data3y,data4y,data5y), axis=1)\n",
    "print Data_Y.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    data_X, Data_Y, test_size=0.25,random_state=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97263L, 4125L)\n",
      "(32422L, 4125L)\n"
     ]
    }
   ],
   "source": [
    "#Normalized Data\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Normalise Each Feature\n",
    "X_train_1=X_train\n",
    "X_test_1=X_test\n",
    "\n",
    "X_train_1=preprocessing.normalize(X_train_1,norm='l2')\n",
    "X_test_1=preprocessing.normalize(X_test_1,norm='l2')\n",
    "\n",
    "\n",
    "        \n",
    "print  X_train_1.shape         \n",
    "print  X_test_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000L, 1L)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lda implementation\n",
    "lda = LDA(n_components=3)\n",
    "train_lda=lda.fit_transform(X_train_1[0:75000,], y_train[0:75000])\n",
    "train_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000L, 1L)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lda=lda.transform(X_test_1[0:25000,])\n",
    "test_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000L, 1L)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000L, 1L)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lda_1=train_lda\n",
    "test_lda_1=test_lda\n",
    "print train_lda_1.shape\n",
    "test_lda_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slf=svm.SVC(kernel='linear').fit(train_lda,y_train[0:75000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.459999999999994"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slf.score(test_lda,y_test[0:25000])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20778L, 1L)\n",
      "(2L,)\n",
      "(1L, 1L)\n"
     ]
    }
   ],
   "source": [
    "ty=slf.support_vectors_\n",
    "print ty.shape\n",
    "ay=slf.n_support_\n",
    "print ay.shape \n",
    "ab=slf.coef_\n",
    "print ab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=2,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000,n_jobs=2)\n",
    "clf.fit(train_lda_1,y_train[0:75000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 0 (1.000000)\n"
     ]
    }
   ],
   "source": [
    "importances = clf.feature_importances_\n",
    "importances.shape\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(train_lda_1.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.012"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_lda_1,y_test[0:25000])*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
